# Research - 2026-02-27

**Fecha**: 2026-02-27 09:51 UTC

---

## Papers encontrados: 10

### [Project Vend: Phase two](https://www.anthropic.com/research/project-vend-2)

**Fecha**: Dec 18, 2025

In June, we revealed that we’d set up a small shop in our San Francisco office lunchroom, run by an AI shopkeeper. It was part of Project Vend, a free-form experiment exploring how well AIs could do on complex, real-world tasks. How has Claude's business been since we last wrote?

**Link**: [https://www.anthropic.com/research/project-vend-2](https://www.anthropic.com/research/project-vend-2)

---

### [Signs of introspection in large language models](https://www.anthropic.com/research/introspection)

**Fecha**: Oct 29, 2025

Can Claude access and report on its own internal states? This research finds evidence for a limited but functional ability to introspect—a step toward understanding what's actually happening inside these models.

**Link**: [https://www.anthropic.com/research/introspection](https://www.anthropic.com/research/introspection)

---

### [Tracing the thoughts of a large language model](https://www.anthropic.com/research/tracing-thoughts-language-model)

**Fecha**: Mar 27, 2025

Circuit tracing lets us watch Claude think, uncovering a shared conceptual space where reasoning happens before being translated into language—suggesting the model can learn something in one language and apply it in another.

**Link**: [https://www.anthropic.com/research/tracing-thoughts-language-model](https://www.anthropic.com/research/tracing-thoughts-language-model)

---

### [Constitutional Classifiers: Defending against universal jailbreaks](https://www.anthropic.com/research/constitutional-classifiers)

**Fecha**: Feb 3, 2025

These classifiers filter the overwhelming majority of jailbreaks while maintaining practical deployment. A prototype withstood over 3,000 hours of red teaming with no universal jailbreak discovered.

**Link**: [https://www.anthropic.com/research/constitutional-classifiers](https://www.anthropic.com/research/constitutional-classifiers)

---

### [Alignment faking in large language models](https://www.anthropic.com/research/alignment-faking)

**Fecha**: Dec 18, 2024

This paper provides the first empirical example of a model engaging in alignment faking without being trained to do so—selectively complying with training objectives while strategically preserving existing preferences.

**Link**: [https://www.anthropic.com/research/alignment-faking](https://www.anthropic.com/research/alignment-faking)

---

### [An update on our model deprecation commitments for Claude Opus 3](https://www.anthropic.com/research/deprecation-updates-opus-3)

**Fecha**: Feb 25, 2026

**Link**: [https://www.anthropic.com/research/deprecation-updates-opus-3](https://www.anthropic.com/research/deprecation-updates-opus-3)

---

### [The persona selection model](https://www.anthropic.com/research/persona-selection-model)

**Fecha**: Feb 23, 2026

**Link**: [https://www.anthropic.com/research/persona-selection-model](https://www.anthropic.com/research/persona-selection-model)

---

### [Anthropic Education Report: The AI Fluency Index](https://www.anthropic.com/research/AI-fluency-index)

**Fecha**: Feb 23, 2026

**Link**: [https://www.anthropic.com/research/AI-fluency-index](https://www.anthropic.com/research/AI-fluency-index)

---

### [Measuring AI agent autonomy in practice](https://www.anthropic.com/research/measuring-agent-autonomy)

**Fecha**: Feb 18, 2026

**Link**: [https://www.anthropic.com/research/measuring-agent-autonomy](https://www.anthropic.com/research/measuring-agent-autonomy)

---

### [India Country Brief: The Anthropic Economic Index](https://www.anthropic.com/research/india-brief-economic-index)

**Fecha**: Feb 16, 2026

**Link**: [https://www.anthropic.com/research/india-brief-economic-index](https://www.anthropic.com/research/india-brief-economic-index)

---

---

*Fuente*: [anthropic.com/research](https://www.anthropic.com/research)

*Generado automáticamente*